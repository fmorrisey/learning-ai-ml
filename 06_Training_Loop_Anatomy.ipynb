{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ffc78e7",
   "metadata": {},
   "source": [
    "# 06 — Training Loop Anatomy (from zero, explained)\n",
    "**Goal:** demystify the training loop by building it piece-by-piece with a tiny neural net on a toy classification problem.\n",
    "\n",
    "You’ll learn:\n",
    "- What *tensors*, *parameters*, *gradients*, *loss*, *optimizer*, and *backprop* actually **do** (in plain terms).\n",
    "- How to structure a **Dataset** and use a **DataLoader** (batches, shuffling, workers).\n",
    "- The difference between `model.train()` and `model.eval()`.\n",
    "- Why we call `optimizer.zero_grad()` and then `loss.backward()` and `optimizer.step()`.\n",
    "- How to **debug shapes** when things break.\n",
    "- How to add **mixed precision** (AMP), **validation**, **checkpoints**, **LR scheduler**, and **gradient clipping**.\n",
    "- How to make runs **reproducible** (seeds).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d932275b",
   "metadata": {},
   "source": [
    "## 0) Imports, device, and reproducibility\n",
    "**Analogy (Frontend):** Think of this as wiring up your build pipeline and setting your env (dev vs prod).\n",
    "- **Device** = where tensors live (`cpu` or `cuda`).  \n",
    "- **Seed** = makes randomness repeatable so results are comparable while you learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc45808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, os, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Device selection\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "# Reproducibility (good defaults while learning)\n",
    "SEED = 42\n",
    "random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "torch.use_deterministic_algorithms(False)  # keep some speed; flip True if you need strict determinism\n",
    "torch.backends.cudnn.benchmark = True      # autotune convs for speed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3905c6",
   "metadata": {},
   "source": [
    "## 1) A tiny **Dataset** (toy classification)\n",
    "We’ll synthesize 2D points from two classes (think: dots on a plane) so there’s nothing to download.\n",
    "\n",
    "**Key terms:**\n",
    "- **Dataset**: an object that knows how to return one `(input, target)` pair by index.\n",
    "- **DataLoader**: batches + shuffles data and optionally uses background workers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b65c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyBlobs(Dataset):\n",
    "    \"\"\"Two Gaussian blobs in 2D, labeled 0/1.\"\"\"\n",
    "    def __init__(self, n_per_class=800, gap=2.5, std=1.0):\n",
    "        # class 0 centered at (-gap, 0), class 1 at (+gap, 0)\n",
    "        c0 = torch.randn(n_per_class, 2) * std + torch.tensor([-gap, 0.0])\n",
    "        c1 = torch.randn(n_per_class, 2) * std + torch.tensor([+gap, 0.0])\n",
    "        x = torch.cat([c0, c1], dim=0)\n",
    "        y = torch.cat([torch.zeros(n_per_class, dtype=torch.long),\n",
    "                       torch.ones(n_per_class,  dtype=torch.long)], dim=0)\n",
    "        # shuffle once here\n",
    "        idx = torch.randperm(x.size(0))\n",
    "        self.x, self.y = x[idx], y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.size(0)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "\n",
    "train_ds = ToyBlobs(n_per_class=1000)\n",
    "val_ds   = ToyBlobs(n_per_class=200)\n",
    "\n",
    "# DataLoaders: batch_size controls VRAM usage; pin_memory helps GPU transfers.\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "len(train_ds), len(val_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe91280b",
   "metadata": {},
   "source": [
    "> **Shape Debug Tip:** Everywhere, keep an eye on `tensor.shape`.\n",
    "If something breaks, print shapes through your pipeline until you find the mismatch—just like inspecting props through a component tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9769068e",
   "metadata": {},
   "source": [
    "## 2) Define a tiny model (**nn.Module**)\n",
    "**Analogy (Frontend):** A model is a component tree with learnable weights. The `forward()` method is your render function.\n",
    "\n",
    "We’ll use a **2-layer MLP**:\n",
    "- Input: 2 features (x & y)\n",
    "- Hidden: 32 units (ReLU)\n",
    "- Output: 2 logits (class scores)\n",
    "\n",
    "**Terms:**\n",
    "- **Parameters**: Tensors registered as weights/biases (PyTorch tracks them).\n",
    "- **Logits**: Raw scores before softmax; `CrossEntropyLoss` expects logits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0206b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden=32, out_dim=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: [batch, 2]\n",
    "        return self.net(x)\n",
    "\n",
    "model = MLP().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d6b946",
   "metadata": {},
   "source": [
    "## 3) Loss & Optimizer\n",
    "- **Loss**: how wrong we are. We’ll use `CrossEntropyLoss` for 2-class classification.\n",
    "- **Optimizer**: updates parameters to reduce loss using gradients (here: `AdamW`).\n",
    "\n",
    "**The 3-step training micro-loop** (memorize this):\n",
    "1. `optimizer.zero_grad()` → clear old gradients\n",
    "2. `loss.backward()` → backprop: compute new gradients\n",
    "3. `optimizer.step()` → move weights a tiny bit opposite the gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61503459",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.7)  # shrink LR every 20 epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70cbb04",
   "metadata": {},
   "source": [
    "## 4) Training and Validation loops (with AMP)\n",
    "- `model.train()` enables dropout/batchnorm training behavior.\n",
    "- `model.eval()` switches them off and we wrap in `torch.no_grad()` for speed.\n",
    "- **AMP** (automatic mixed precision) uses `autocast` + `GradScaler` to speed up and lower memory usage on GPUs.\n",
    "\n",
    "We’ll collect: `loss`, `accuracy`. Accuracy is easy for classification: compare `argmax` of logits with labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "scaler = GradScaler(enabled=(device=='cuda'))\n",
    "\n",
    "def accuracy_from_logits(logits, y):\n",
    "    pred = logits.argmax(dim=1)\n",
    "    return (pred == y).float().mean().item()\n",
    "\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "\n",
    "        with autocast(enabled=(device=='cuda')):\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        # optional: gradient clipping to stabilize\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_acc  += accuracy_from_logits(logits, yb) * bs\n",
    "        n += bs\n",
    "    return total_loss/n, total_acc/n\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        bs = xb.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_acc  += accuracy_from_logits(logits, yb) * bs\n",
    "        n += bs\n",
    "    return total_loss/n, total_acc/n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603f6106",
   "metadata": {},
   "source": [
    "## 5) Run it\n",
    "Watch loss ↓ and accuracy ↑ over epochs. If things get worse, that’s a bug or too-large learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8458b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "best_val_acc, best_state = 0.0, None\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader)\n",
    "    va_loss, va_acc = validate(model, val_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    if va_acc > best_val_acc:\n",
    "        best_val_acc = va_acc\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    print(f\"epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.3f} | val loss {va_loss:.4f} acc {va_acc:.3f} | lr {scheduler.get_last_lr()[0]:.5f}\")\n",
    "\n",
    "# Save best checkpoint\n",
    "if best_state is not None:\n",
    "    torch.save(best_state, 'toy_mlp_best.pt')\n",
    "    print('Saved best checkpoint -> toy_mlp_best.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e47990",
   "metadata": {},
   "source": [
    "## 6) Load & use the checkpoint (inference)\n",
    "**Inference** = forward pass only (no gradients). Here we’ll reload and score the validation set again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91d8928",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def load_and_eval(path='toy_mlp_best.pt'):\n",
    "    m = MLP().to(device)\n",
    "    m.load_state_dict(torch.load(path, map_location=device))\n",
    "    loss, acc = validate(m, val_loader)\n",
    "    print(f'Reloaded model -> val loss {loss:.4f} acc {acc:.3f}')\n",
    "    return m\n",
    "\n",
    "_ = load_and_eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19afefd4",
   "metadata": {},
   "source": [
    "## 7) Common questions (quick answers)\n",
    "- **Why zero gradients?** Gradients accumulate by default. If you don’t clear them, each `backward()` adds to the previous one.\n",
    "- **Why `train()` vs `eval()`?** Some layers behave differently during training (dropout, batchnorm). Use the right mode.\n",
    "- **Why AMP?** Faster training + less VRAM on GPU with near-identical accuracy for most models.\n",
    "- **Why a scheduler?** Big learning rates help early, smaller ones help refine later. Schedulers automate that.\n",
    "- **Why clip gradients?** Prevents exploding gradients in some models; a safe default limit helps stability.\n",
    "\n",
    "**Memorize this mini template:**\n",
    "```python\n",
    "for xb, yb in train_loader:\n",
    "    with autocast():\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer); clip_grad_norm_(model.parameters(), 5.0)\n",
    "    scaler.step(optimizer); scaler.update()\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
