{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e59ee07b",
   "metadata": {},
   "source": [
    "# 07 — Datasets, Transforms, and DataLoader (Vision)\n",
    "**Goal:** master the input pipeline for image tasks so you can swap datasets and augmentations without touching model code.\n",
    "\n",
    "You’ll learn:\n",
    "- How **torchvision.datasets** works (MNIST/CIFAR-10 as examples).\n",
    "- Why we **normalize** and how **data augmentation** reduces overfitting.\n",
    "- DataLoader knobs: `batch_size`, `shuffle`, `num_workers`, `pin_memory`, `persistent_workers`.\n",
    "- A clean train/val split you can reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284631ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "data_root = './data'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b393d40d",
   "metadata": {},
   "source": [
    "## 1) Transforms 101\n",
    "- **ToTensor**: converts H×W×C `[0,255]` images to C×H×W floats `[0,1]`.\n",
    "- **Normalize**: standardizes channels: `(x - mean) / std`.\n",
    "- **Augmentation** (train only): flips, crops, color jitter, etc. (Don’t augment validation/test!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9eeba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST (grayscale) normalization constants (precomputed on the dataset)\n",
    "mnist_mean, mnist_std = (0.1307,), (0.3081,)\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05,0.05), scale=(0.95,1.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mnist_mean, mnist_std),\n",
    "])\n",
    "\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mnist_mean, mnist_std),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba74c2ca",
   "metadata": {},
   "source": [
    "## 2) Load a dataset\n",
    "`download=True` will fetch it the first time. Switch to CIFAR-10 by replacing `MNIST` + norms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = datasets.MNIST(root=data_root, train=True,  transform=train_tf, download=True)\n",
    "test_ds    = datasets.MNIST(root=data_root, train=False, transform=test_tf, download=True)\n",
    "\n",
    "# Make a small validation split from the training set\n",
    "val_size = 5000\n",
    "train_size = len(train_full) - val_size\n",
    "train_ds, val_ds = random_split(train_full, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae37c9",
   "metadata": {},
   "source": [
    "## 3) Build DataLoaders (performance knobs)\n",
    "- **batch_size**: bigger = faster but uses more VRAM.\n",
    "- **num_workers**: use >0 to load/transform in parallel (try 2–4 first).\n",
    "- **pin_memory=True**: faster host→GPU transfer.\n",
    "- **persistent_workers=True**: keeps workers alive between epochs for speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3acfec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True,  num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False, num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=256, shuffle=False, num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "for name, loader in [('train', train_loader), ('val', val_loader), ('test', test_loader)]:\n",
    "    xb, yb = next(iter(loader))\n",
    "    print(f\"{name:>5} batch -> x:{tuple(xb.shape)}  y:{tuple(yb.shape)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a41a56",
   "metadata": {},
   "source": [
    "## 4) Plug-and-play with your model\n",
    "Drop in any CNN (e.g., the MNIST net from earlier). The loaders are interchangeable as long as shapes match what your model expects.\n",
    "\n",
    "**Quick exercise:** copy your MNIST CNN here, train for 3 epochs using these loaders, then try:\n",
    "- Doubling `batch_size` (watch VRAM via `nvidia-smi`).\n",
    "- Increasing `num_workers` (should improve data throughput).\n",
    "- Removing augmentation (train accuracy up, val accuracy down = overfitting).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
