{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d767a3ce",
   "metadata": {},
   "source": [
    "# 09 — Losses, Optimizers, and Learning-Rate Schedulers\n",
    "**Goal:** understand *why* these knobs exist and how to pick sane defaults without heavy math.\n",
    "\n",
    "**We’ll cover:**\n",
    "- Loss functions (MSE, CrossEntropy) and when to use them\n",
    "- Optimizers (SGD, Adam, AdamW): intuition + trade-offs\n",
    "- Learning-rate schedules (StepLR, CosineAnnealing, OneCycle)\n",
    "- Practical defaults you can memorize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b289a",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b2c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, OneCycleLR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4abd1bf",
   "metadata": {},
   "source": [
    "## 1) Loss function chooser\n",
    "- **Regression** (predict a number): use `MSELoss`.\n",
    "- **Classification** with integer labels: use `CrossEntropyLoss`.\n",
    "- **Multi-label** (many independent yes/no): use `BCEWithLogitsLoss`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aff59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.randn(16, 10)\n",
    "y_cls  = torch.randint(0, 10, (16,))\n",
    "y_reg  = torch.randn(16, 1)\n",
    "print('CrossEntropy:', nn.CrossEntropyLoss()(logits, y_cls).item())\n",
    "pred   = torch.randn(16,1)\n",
    "print('MSE:', nn.MSELoss()(pred, y_reg).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae983ed7",
   "metadata": {},
   "source": [
    "## 2) Optimizers in one paragraph each\n",
    "- **SGD**: like walking downhill with short, straight steps. Needs tuning but very solid.\n",
    "- **Adam**: adapts step sizes per-parameter; fast to good results, can overfit.\n",
    "- **AdamW**: Adam + correct weight decay (preferred default for many tasks).\n",
    "**Rule of thumb:** start with `AdamW(lr=1e-3, weight_decay=1e-2)` for small models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc47475",
   "metadata": {},
   "source": [
    "## 3) Learning-rate schedules\n",
    "- **StepLR**: drop LR every *k* epochs → stable, simple.\n",
    "- **CosineAnnealingLR**: smooth decay → often works slightly better.\n",
    "- **OneCycleLR**: good for faster convergence; needs `max_lr` and total steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ac7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(32,64), nn.ReLU(), nn.Linear(64,10))\n",
    "opt = AdamW(model.parameters(), lr=1e-3)\n",
    "steps_per_epoch = 100\n",
    "sched = OneCycleLR(opt, max_lr=3e-3, epochs=5, steps_per_epoch=steps_per_epoch)\n",
    "for epoch in range(5):\n",
    "    for step in range(steps_per_epoch):\n",
    "        x = torch.randn(64,32); y = torch.randint(0,10,(64,))\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        opt.zero_grad(); loss.backward(); opt.step(); sched.step()\n",
    "    print('epoch', epoch+1, 'done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77254564",
   "metadata": {},
   "source": [
    "## 4) Practical defaults cheat-sheet\n",
    "- **Batch size**: start 64–256 (lower if VRAM errors)\n",
    "- **Optimizer**: `AdamW(lr=1e-3, weight_decay=1e-2)`\n",
    "- **LR schedule**: `StepLR(step_size=20, gamma=0.7)` or `CosineAnnealingLR(T_max=EPOCHS)`\n",
    "- **AMP**: use it on GPU\n",
    "- **Gradient clipping**: `clip_grad_norm_(params, 1–5)` if unstable\n",
    "- **Early stopping**: keep the best validation checkpoint\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}